{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fcddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class InputEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, rnn_type='LSTM',\n",
    "                 dropout=0.2, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.is_bidirectional = bidirectional\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        rnn_cls = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[rnn_type]\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim // self.num_directions,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return hidden\n",
    "\n",
    "\n",
    "class OutputDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, rnn_type='LSTM',\n",
    "                 dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn_type = rnn_type\n",
    "\n",
    "        rnn_cls = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[rnn_type]\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_token, hidden_state):\n",
    "        embedded = self.embedding(input_token.unsqueeze(1))  # (B, 1, E)\n",
    "        rnn_output, hidden = self.rnn(embedded, hidden_state)\n",
    "        logits = self.fc_out(rnn_output.squeeze(1))  # (B, V)\n",
    "        return logits, hidden\n",
    "\n",
    "\n",
    "class TransliterationModel(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, embed_dim, hidden_dim,\n",
    "                 enc_layers, dec_layers, rnn_type='LSTM', dropout=0.2, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.encoder = InputEncoder(input_vocab_size, embed_dim, hidden_dim,\n",
    "                                    enc_layers, rnn_type, dropout, bidirectional)\n",
    "        self.decoder = OutputDecoder(output_vocab_size, embed_dim, hidden_dim,\n",
    "                                     dec_layers, rnn_type, dropout)\n",
    "        self.rnn_type = rnn_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.enc_layers = enc_layers\n",
    "        self.dec_layers = dec_layers\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        vocab_size = self.decoder.fc_out.out_features\n",
    "        outputs = torch.zeros(batch_size, tgt_len, vocab_size, device=src.device)\n",
    "\n",
    "        enc_hidden = self.encoder(src)\n",
    "\n",
    "        def merge_bidir_states(state):\n",
    "            return torch.cat([state[::2], state[1::2]], dim=2)\n",
    "\n",
    "        def pad_layers(state, target_layers):\n",
    "            if state.size(0) == target_layers:\n",
    "                return state\n",
    "            pad = torch.zeros(target_layers - state.size(0), *state.shape[1:], device=state.device)\n",
    "            return torch.cat([state, pad], dim=0)\n",
    "\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            h, c = enc_hidden\n",
    "            if self.bidirectional:\n",
    "                h, c = merge_bidir_states(h), merge_bidir_states(c)\n",
    "            h, c = pad_layers(h, self.dec_layers), pad_layers(c, self.dec_layers)\n",
    "            dec_hidden = (h, c)\n",
    "        else:\n",
    "            h = enc_hidden\n",
    "            if self.bidirectional:\n",
    "                h = merge_bidir_states(h)\n",
    "            h = pad_layers(h, self.dec_layers)\n",
    "            dec_hidden = h\n",
    "\n",
    "        dec_input = tgt[:, 0]  # Start token\n",
    "        for t in range(1, tgt_len):\n",
    "            output, dec_hidden = self.decoder(dec_input, dec_hidden)\n",
    "            outputs[:, t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            dec_input = tgt[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "def read_pairs(file_path):\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        return [(line.split('\\t')[1], line.split('\\t')[0]) for line in f.read().strip().split('\\n') if '\\t' in line]\n",
    "\n",
    "def build_vocab_and_prepare_batch(seqs, device):\n",
    "    special_tokens = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "    \n",
    "    # Build character sets\n",
    "    unique_chars_latin = sorted(set(ch for seq in seqs for ch in seq[0]))\n",
    "    unique_chars_dev = sorted(set(ch for seq in seqs for ch in seq[1]))\n",
    "\n",
    "    # Build vocabularies\n",
    "    src_vocab = {ch: idx + len(special_tokens) for idx, ch in enumerate(unique_chars_latin)}\n",
    "    tgt_vocab = {ch: idx + len(special_tokens) for idx, ch in enumerate(unique_chars_dev)}\n",
    "    src_vocab.update(special_tokens)\n",
    "    tgt_vocab.update(special_tokens)\n",
    "\n",
    "    idx2src = {idx: ch for ch, idx in src_vocab.items()}\n",
    "    idx2tgt = {idx: ch for ch, idx in tgt_vocab.items()}\n",
    "\n",
    "    def encode_text(seq, vocab):\n",
    "        return [vocab.get(ch, vocab['<unk>']) for ch in seq]\n",
    "\n",
    "    def create_batch(pairs):\n",
    "        src = [torch.tensor(encode_text(x, src_vocab) + [src_vocab['<eos>']]) for x, _ in pairs]\n",
    "        tgt = [torch.tensor([tgt_vocab['<sos>']] + encode_text(y, tgt_vocab) + [tgt_vocab['<eos>']]) for _, y in pairs]\n",
    "        src = pad_sequence(src, batch_first=True, padding_value=src_vocab['<pad>'])\n",
    "        tgt = pad_sequence(tgt, batch_first=True, padding_value=tgt_vocab['<pad>'])\n",
    "        return src.to(device), tgt.to(device)\n",
    "\n",
    "    return src_vocab, idx2src, tgt_vocab, idx2tgt, create_batch, unique_chars_latin, unique_chars_dev\n",
    "\n",
    "def compute_word_level_accuracy(preds, targets, vocab):\n",
    "    sos, eos, pad = vocab['<sos>'], vocab['<eos>'], vocab['<pad>']\n",
    "    preds = preds.tolist()\n",
    "    targets = targets.tolist()\n",
    "    correct = 0\n",
    "    for p, t in zip(preds, targets):\n",
    "        p = [x for x in p if x != pad and x != eos]\n",
    "        t = [x for x in t if x != pad and x != eos]\n",
    "        if p == t:\n",
    "            correct += 1\n",
    "    return correct / len(preds) * 100\n",
    "\n",
    "def run_training():\n",
    "    # Initialize wandb config\n",
    "    wandb.init()\n",
    "    cfg = wandb.config\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load and prepare data\n",
    "    train_path = \"dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
    "    dev_path = \"dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\n",
    "    train_set = read_pairs(train_path)\n",
    "    dev_set = read_pairs(dev_path)\n",
    "\n",
    "    src_vocab, idx2src, tgt_vocab, idx2tgt, create_batch, _, _ = build_vocab_and_prepare_batch(train_set, device)\n",
    "\n",
    "    # Initialize model, optimizer, criterion\n",
    "    model = TransliterationModel(\n",
    "        len(src_vocab), len(tgt_vocab), cfg.embedding_size, cfg.hidden_size,\n",
    "        cfg.enc_layers, cfg.dec_layers, cfg.rnn_type, cfg.dropout_rate,\n",
    "        cfg.is_bidirectional\n",
    "    ).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab['<pad>'])\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(cfg.epochs):\n",
    "        model.train()\n",
    "        total_loss, total_acc = 0, 0\n",
    "        random.shuffle(train_set)\n",
    "\n",
    "        for i in range(0, len(train_set), cfg.batch_size):\n",
    "            batch = train_set[i:i+cfg.batch_size]\n",
    "            src, tgt = create_batch(batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(src, tgt, cfg.teacher_forcing_prob)\n",
    "\n",
    "            loss = criterion(outputs[:, 1:].reshape(-1, outputs.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = outputs.argmax(-1)\n",
    "            acc = compute_word_level_accuracy(preds[:, 1:], tgt[:, 1:], tgt_vocab)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "\n",
    "        avg_train_loss = total_loss / (len(train_set) // cfg.batch_size)\n",
    "        avg_train_acc = total_acc / (len(train_set) // cfg.batch_size)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        dev_loss, dev_acc = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(dev_set), cfg.batch_size):\n",
    "                batch = dev_set[i:i+cfg.batch_size]\n",
    "                src, tgt = create_batch(batch)\n",
    "                outputs = model(src, tgt, 0)\n",
    "                loss = criterion(outputs[:, 1:].reshape(-1, outputs.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "\n",
    "                preds = outputs.argmax(-1)\n",
    "                acc = compute_word_level_accuracy(preds[:, 1:], tgt[:, 1:], tgt_vocab)\n",
    "\n",
    "                dev_loss += loss.item()\n",
    "                dev_acc += acc\n",
    "\n",
    "        avg_dev_loss = dev_loss / (len(dev_set) // cfg.batch_size)\n",
    "        avg_dev_acc = dev_acc / (len(dev_set) // cfg.batch_size)\n",
    "\n",
    "        # Logging\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch + 1,\n",
    "            \"Train Loss\": avg_train_loss,\n",
    "            \"Train Accuracy\": avg_train_acc,\n",
    "            \"Validation Loss\": avg_dev_loss,\n",
    "            \"Validation Accuracy\": avg_dev_acc,\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{cfg.epochs} | Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.2f}% | Val Loss: {avg_dev_loss:.4f}, Val Acc: {avg_dev_acc:.2f}%\")\n",
    "\n",
    "    wandb.finish()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd9721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: q84vgxva\n",
      "Sweep URL: https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/dakshina_transliteration/sweeps/q84vgxva\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: hzvyxt6c with config:\n",
      "wandb: \tbatch_size: 32\n",
      "wandb: \tdec_layers: 2\n",
      "wandb: \tdropout_rate: 0.2\n",
      "wandb: \tembedding_size: 128\n",
      "wandb: \tenc_layers: 1\n",
      "wandb: \tepochs: 10\n",
      "wandb: \thidden_size: 64\n",
      "wandb: \tis_bidirectional: False\n",
      "wandb: \tlearning_rate: 0.001\n",
      "wandb: \toptimizer: nadam\n",
      "wandb: \trnn_type: GRU\n",
      "wandb: \tteacher_forcing_prob: 0.7\n",
      "wandb: Currently logged in as: harshtrivs (harshtrivs-indian-institute-of-technology-madras) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\code\\DL\\da6401_assignment3\\wandb\\run-20250518_172820-hzvyxt6c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/dakshina_transliteration/runs/hzvyxt6c' target=\"_blank\">different-sweep-1</a></strong> to <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/dakshina_transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/dakshina_transliteration/sweeps/q84vgxva' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/dakshina_transliteration/sweeps/q84vgxva</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/dakshina_transliteration' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/dakshina_transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/dakshina_transliteration/sweeps/q84vgxva' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/dakshina_transliteration/sweeps/q84vgxva</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/dakshina_transliteration/runs/hzvyxt6c' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/dakshina_transliteration/runs/hzvyxt6c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 2.6848, Train Acc: 0.06% | Val Loss: 2.6699, Val Acc: 0.30%\n",
      "Epoch 2/10 | Train Loss: 1.8966, Train Acc: 1.65% | Val Loss: 2.2014, Val Acc: 4.24%\n",
      "Epoch 3/10 | Train Loss: 1.5230, Train Acc: 5.39% | Val Loss: 1.9162, Val Acc: 8.95%\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {'name': 'Validation Accuracy', 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'embedding_size': {'values': [32, 64, 128, 256]},\n",
    "        'hidden_size': {'values': [64, 128, 256]},\n",
    "        'enc_layers': {'values': [1, 2, 3]},\n",
    "        'dec_layers': {'values': [1, 2, 3]},\n",
    "        'rnn_type': {'values': ['GRU', 'LSTM','RNN']},\n",
    "        'dropout_rate': {'values': [0.2, 0.3]},\n",
    "        'batch_size': {'values': [32, 64]},\n",
    "        'epochs': {\n",
    "            'values': [5, 10]},\n",
    "        'is_bidirectional': {'values': [False, True]},\n",
    "        'learning_rate': {'values': [0.001, 0.002]},\n",
    "        'optimizer': {'values': ['adam', 'nadam']},\n",
    "        'teacher_forcing_prob': {'values': [0.2, 0.5, 0.7]}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"dakshina_transliteration\")\n",
    "wandb.agent(sweep_id, function=run_training, count=50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
