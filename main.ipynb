{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fcddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, rnn_type='LSTM',\n",
    "                 dropout=0.2, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.is_bidirectional = bidirectional\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        rnn_cls = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[rnn_type]\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim // self.num_directions,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, rnn_type='LSTM',\n",
    "                 dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn_type = rnn_type\n",
    "\n",
    "        rnn_cls = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[rnn_type]\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_token, hidden_state):\n",
    "        embedded = self.embedding(input_token.unsqueeze(1))  # (B, 1, E)\n",
    "        rnn_output, hidden = self.rnn(embedded, hidden_state)\n",
    "        logits = self.fc_out(rnn_output.squeeze(1))  # (B, V)\n",
    "        return logits, hidden\n",
    "\n",
    "\n",
    "class TransliterationModel(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, embed_dim, hidden_dim,\n",
    "                 enc_layers, dec_layers, rnn_type='LSTM', dropout=0.2, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_vocab_size, embed_dim, hidden_dim,\n",
    "                                    enc_layers, rnn_type, dropout, bidirectional)\n",
    "        self.decoder = Decoder(output_vocab_size, embed_dim, hidden_dim,\n",
    "                                     dec_layers, rnn_type, dropout)\n",
    "        self.rnn_type = rnn_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.enc_layers = enc_layers\n",
    "        self.dec_layers = dec_layers\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        vocab_size = self.decoder.fc_out.out_features\n",
    "        outputs = torch.zeros(batch_size, tgt_len, vocab_size, device=src.device)\n",
    "\n",
    "        enc_hidden = self.encoder(src)\n",
    "\n",
    "        def merge_bidir_states(state):\n",
    "            return torch.cat([state[::2], state[1::2]], dim=2)\n",
    "\n",
    "        def pad_layers(state, target_layers):\n",
    "            if state.size(0) == target_layers:\n",
    "                return state\n",
    "            pad = torch.zeros(target_layers - state.size(0), *state.shape[1:], device=state.device)\n",
    "            return torch.cat([state, pad], dim=0)\n",
    "\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            h, c = enc_hidden\n",
    "            if self.bidirectional:\n",
    "                h, c = merge_bidir_states(h), merge_bidir_states(c)\n",
    "            h, c = pad_layers(h, self.dec_layers), pad_layers(c, self.dec_layers)\n",
    "            dec_hidden = (h, c)\n",
    "        else:\n",
    "            h = enc_hidden\n",
    "            if self.bidirectional:\n",
    "                h = merge_bidir_states(h)\n",
    "            h = pad_layers(h, self.dec_layers)\n",
    "            dec_hidden = h\n",
    "\n",
    "        dec_input = tgt[:, 0]  # Start token\n",
    "        for t in range(1, tgt_len):\n",
    "            output, dec_hidden = self.decoder(dec_input, dec_hidden)\n",
    "            outputs[:, t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            dec_input = tgt[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "def read_pairs(file_path):\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        return [(line.split('\\t')[1], line.split('\\t')[0]) for line in f.read().strip().split('\\n') if '\\t' in line]\n",
    "\n",
    "def build_vocab_and_prepare_batch(seqs, device):\n",
    "    special_tokens = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "    \n",
    "    # Build character sets\n",
    "    unique_chars_latin = sorted(set(ch for seq in seqs for ch in seq[0]))\n",
    "    unique_chars_dev = sorted(set(ch for seq in seqs for ch in seq[1]))\n",
    "\n",
    "    # Build vocabularies\n",
    "    src_vocab = {ch: idx + len(special_tokens) for idx, ch in enumerate(unique_chars_latin)}\n",
    "    tgt_vocab = {ch: idx + len(special_tokens) for idx, ch in enumerate(unique_chars_dev)}\n",
    "    src_vocab.update(special_tokens)\n",
    "    tgt_vocab.update(special_tokens)\n",
    "\n",
    "    idx2src = {idx: ch for ch, idx in src_vocab.items()}\n",
    "    idx2tgt = {idx: ch for ch, idx in tgt_vocab.items()}\n",
    "\n",
    "    def encode_text(seq, vocab):\n",
    "        return [vocab.get(ch, vocab['<unk>']) for ch in seq]\n",
    "\n",
    "    def create_batch(pairs):\n",
    "        src = [torch.tensor(encode_text(x, src_vocab) + [src_vocab['<eos>']]) for x, _ in pairs]\n",
    "        tgt = [torch.tensor([tgt_vocab['<sos>']] + encode_text(y, tgt_vocab) + [tgt_vocab['<eos>']]) for _, y in pairs]\n",
    "        src = pad_sequence(src, batch_first=True, padding_value=src_vocab['<pad>'])\n",
    "        tgt = pad_sequence(tgt, batch_first=True, padding_value=tgt_vocab['<pad>'])\n",
    "        return src.to(device), tgt.to(device)\n",
    "\n",
    "    return src_vocab, idx2src, tgt_vocab, idx2tgt, create_batch, unique_chars_latin, unique_chars_dev\n",
    "\n",
    "def compute_word_level_accuracy(preds, targets, vocab):\n",
    "    sos, eos, pad = vocab['<sos>'], vocab['<eos>'], vocab['<pad>']\n",
    "    preds = preds.tolist()\n",
    "    targets = targets.tolist()\n",
    "    correct = 0\n",
    "    for p, t in zip(preds, targets):\n",
    "        p = [x for x in p if x != pad and x != eos]\n",
    "        t = [x for x in t if x != pad and x != eos]\n",
    "        if p == t:\n",
    "            correct += 1\n",
    "    return correct / len(preds) * 100\n",
    "\n",
    "def run_training():\n",
    "    # Initialize wandb config\n",
    "    wandb.init()\n",
    "    cfg = wandb.config\n",
    "    wandb.run.name = (\n",
    "    f\"es_{cfg.embedding_size}_hs_{cfg.hidden_size}_\"\n",
    "    f\"enc_{cfg.enc_layers}_dec_{cfg.dec_layers}_\"\n",
    "    f\"rnn_{cfg.rnn_type}_dropout_{cfg.dropout_rate}_\"\n",
    "    f\"bidirectional_{cfg.is_bidirectional}_\"\n",
    "    f\"lr_{cfg.learning_rate}_bs_{cfg.batch_size}_\"\n",
    "    f\"epochs_{cfg.epochs}_tfp_{cfg.teacher_forcing_prob}_\"\n",
    "    f\"beam_size_{cfg.beam_size}\"\n",
    "    )\n",
    "\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load and prepare data\n",
    "    train_path = \"dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
    "    dev_path = \"dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\n",
    "    train_set = read_pairs(train_path)\n",
    "    dev_set = read_pairs(dev_path)\n",
    "\n",
    "    src_vocab, idx2src, tgt_vocab, idx2tgt, create_batch, _, _ = build_vocab_and_prepare_batch(train_set, device)\n",
    "\n",
    "    # Initialize model, optimizer, criterion\n",
    "    model = TransliterationModel(\n",
    "        len(src_vocab), len(tgt_vocab), cfg.embedding_size, cfg.hidden_size,\n",
    "        cfg.enc_layers, cfg.dec_layers, cfg.rnn_type, cfg.dropout_rate,\n",
    "        cfg.is_bidirectional\n",
    "    ).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab['<pad>'])\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(cfg.epochs):\n",
    "        model.train()\n",
    "        total_loss, total_acc = 0, 0\n",
    "        random.shuffle(train_set)\n",
    "\n",
    "        for i in range(0, len(train_set), cfg.batch_size):\n",
    "            batch = train_set[i:i+cfg.batch_size]\n",
    "            src, tgt = create_batch(batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(src, tgt, cfg.teacher_forcing_prob)\n",
    "\n",
    "            loss = criterion(outputs[:, 1:].reshape(-1, outputs.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = outputs.argmax(-1)\n",
    "            acc = compute_word_level_accuracy(preds[:, 1:], tgt[:, 1:], tgt_vocab)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "\n",
    "        avg_train_loss = total_loss / (len(train_set) // cfg.batch_size)\n",
    "        avg_train_acc = total_acc / (len(train_set) // cfg.batch_size)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        dev_loss, dev_acc = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(dev_set), cfg.batch_size):\n",
    "                batch = dev_set[i:i+cfg.batch_size]\n",
    "                src, tgt = create_batch(batch)\n",
    "                outputs = model(src, tgt, 0)\n",
    "                loss = criterion(outputs[:, 1:].reshape(-1, outputs.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "\n",
    "                preds = outputs.argmax(-1)\n",
    "                acc = compute_word_level_accuracy(preds[:, 1:], tgt[:, 1:], tgt_vocab)\n",
    "\n",
    "                dev_loss += loss.item()\n",
    "                dev_acc += acc\n",
    "\n",
    "        avg_dev_loss = dev_loss / (len(dev_set) // cfg.batch_size)\n",
    "        avg_dev_acc = dev_acc / (len(dev_set) // cfg.batch_size)\n",
    "\n",
    "        # Logging\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch + 1,\n",
    "            \"Train Loss\": avg_train_loss,\n",
    "            \"Train Accuracy\": avg_train_acc,\n",
    "            \"Validation Loss\": avg_dev_loss,\n",
    "            \"Validation Accuracy\": avg_dev_acc,\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{cfg.epochs} | Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.2f}% | Val Loss: {avg_dev_loss:.4f}, Val Acc: {avg_dev_acc:.2f}%\")\n",
    "\n",
    "    wandb.finish()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd9721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep_config = {\n",
    "#     'method': 'bayes',\n",
    "#     'metric': {'name': 'Validation Accuracy', 'goal': 'maximize'},\n",
    "#     'parameters': {\n",
    "#         'embedding_size': {'values': [128, 256]},\n",
    "#         'hidden_size': {'values': [128, 256]},\n",
    "#         'enc_layers': {'values': [2, 3]},\n",
    "#         'dec_layers': {'values': [2, 3]},\n",
    "#         'rnn_type': {'values': ['GRU', 'LSTM','RNN']},\n",
    "#         'dropout_rate': {'values': [0.2, 0.3]},\n",
    "#         'batch_size': {'values': [32, 64]},\n",
    "#         'epochs': {\n",
    "#             'values': [5, 10]},\n",
    "#         'is_bidirectional': {'values': [False, True]},\n",
    "#         'learning_rate': {'values': [0.001, 0.0001]},\n",
    "#         'optimizer': {'values': ['adam', 'nadam']},\n",
    "#         'teacher_forcing_prob': {'values': [0.2, 0.5, 0.7]},\n",
    "#         'beam_size': {'values': [1,3,5]},\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep_config, project=\"dakshina_transliteration\")\n",
    "# wandb.agent(sweep_id, function=run_training, count=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7647825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(cfg):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    wandb.finish()\n",
    "    wandb.init(\n",
    "        project=\"transliteration_evaluation\",\n",
    "        name = 'best_model_test_eval',\n",
    "        resume=\"never\",\n",
    "        reinit=True,\n",
    "        config=cfg\n",
    "    )\n",
    "    # Load and prepare data\n",
    "    model_path = \"best_vanilla_model.pt\"\n",
    "    train_path = \"dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
    "    train_set = read_pairs(train_path)\n",
    "    test_path = \"dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\"\n",
    "    test_set = read_pairs(test_path)\n",
    "\n",
    "    src_vocab, idx2src, tgt_vocab, idx2tgt, create_batch, _, _ = build_vocab_and_prepare_batch(train_set, device)\n",
    "\n",
    "    # Initialize model, optimizer, criterion\n",
    "    model = TransliterationModel(\n",
    "        len(src_vocab), len(tgt_vocab), cfg['embedding_size'], cfg['hidden_size'],\n",
    "        cfg['enc_layers'], cfg['dec_layers'], cfg['rnn_type'], cfg['dropout_rate'],\n",
    "        cfg['is_bidirectional']\n",
    "    ).to(device)\n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"❌ No saved model found, starting training.\")\n",
    "        optimizer = optim.Adam(model.parameters(), lr=cfg['learning_rate'])\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab['<pad>'])\n",
    "        best_acc = 0.0\n",
    "        # Training loop\n",
    "        for epoch in range(cfg['epochs']):\n",
    "            model.train()\n",
    "            total_loss, total_acc = 0, 0\n",
    "            random.shuffle(train_set)\n",
    "\n",
    "            for i in range(0, len(train_set), cfg['batch_size']):\n",
    "                batch = train_set[i:i+cfg['batch_size']]\n",
    "                src, tgt = create_batch(batch)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(src, tgt, cfg['teacher_forcing_prob'])\n",
    "\n",
    "                loss = criterion(outputs[:, 1:].reshape(-1, outputs.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                preds = outputs.argmax(-1)\n",
    "                acc = compute_word_level_accuracy(preds[:, 1:], tgt[:, 1:], tgt_vocab)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_acc += acc\n",
    "\n",
    "            avg_train_loss = total_loss / (len(train_set) // cfg['batch_size'])\n",
    "            avg_train_acc = total_acc / (len(train_set) // cfg['batch_size'])\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{cfg['epochs']} | Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.2f}%\")\n",
    "            wandb.log({\"Train Loss\": avg_train_loss, \"Train Accuracy\": avg_train_acc})\n",
    "\n",
    "            # Save the best model\n",
    "            if avg_train_acc > best_acc:\n",
    "                best_acc = avg_train_acc\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print(f\"💾 Saved new best model at epoch {epoch + 1} with accuracy {best_acc:.2f}%\")\n",
    "        print(f\"Best model saved with accuracy: {best_acc:.2f}%\")\n",
    "\n",
    "    #test the model\n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print(\"✅ Loaded saved model from disk.\")\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(test_set), cfg['batch_size']):\n",
    "            batch = test_set[i:i + cfg['batch_size']]\n",
    "            src, tgt = create_batch(batch)\n",
    "            outputs = model(src, tgt, 0)\n",
    "            preds = outputs.argmax(-1)\n",
    "\n",
    "            for j in range(src.size(0)):\n",
    "                input_seq = ''.join([idx2src.get(idx.item(), '') for idx in src[j] if idx.item() not in [src_vocab['<pad>'], src_vocab['<eos>']]])\n",
    "                target_seq = ''.join([idx2tgt.get(idx.item(), '') for idx in tgt[j][1:] if idx.item() not in [tgt_vocab['<pad>'], tgt_vocab['<eos>']]])\n",
    "                pred_seq = ''.join([idx2tgt.get(idx.item(), '') for idx in preds[j][1:] if idx.item() not in [tgt_vocab['<pad>'], tgt_vocab['<eos>']]])\n",
    "                is_correct = target_seq == pred_seq\n",
    "                predictions.append({'Input': input_seq, 'Target': target_seq, 'Predicted': pred_seq , 'Is_Correct': \"True✅\" if is_correct else \"False❌\"})\n",
    "    predictions = pd.DataFrame(predictions)\n",
    "    overall_acc = (predictions.Is_Correct == \"True✅\").mean()\n",
    "    wandb.log({\"Test Accuracy\": overall_acc})\n",
    "    table = wandb.Table(dataframe=predictions)\n",
    "    wandb.log({\"predictions_table_vanilla\": table})\n",
    "    # finish run\n",
    "    wandb.finish()\n",
    "    predictions.to_csv('predictions_vanilla.csv', index=False)\n",
    "    print(f\"Saved {len(predictions)} rows, eval accuracy = {overall_acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1534a109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: harshtrivs (harshtrivs-indian-institute-of-technology-madras) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\code\\DL\\da6401_assignment3\\wandb\\run-20250519_070301-3ae1svie</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/transliteration_evaluation/runs/3ae1svie' target=\"_blank\">best_model_test_eval</a></strong> to <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/transliteration_evaluation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/transliteration_evaluation' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/transliteration_evaluation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/transliteration_evaluation/runs/3ae1svie' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/transliteration_evaluation/runs/3ae1svie</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ No saved model found, starting training.\n",
      "Epoch 1/10 | Train Loss: 1.6193, Train Acc: 7.72%\n",
      "💾 Saved new best model at epoch 1 with accuracy 7.72%\n",
      "Epoch 2/10 | Train Loss: 0.7582, Train Acc: 19.03%\n",
      "💾 Saved new best model at epoch 2 with accuracy 19.03%\n",
      "Epoch 3/10 | Train Loss: 0.6072, Train Acc: 25.61%\n",
      "💾 Saved new best model at epoch 3 with accuracy 25.61%\n",
      "Epoch 4/10 | Train Loss: 0.5297, Train Acc: 28.85%\n",
      "💾 Saved new best model at epoch 4 with accuracy 28.85%\n",
      "Epoch 5/10 | Train Loss: 0.4836, Train Acc: 31.31%\n",
      "💾 Saved new best model at epoch 5 with accuracy 31.31%\n",
      "Epoch 6/10 | Train Loss: 0.4363, Train Acc: 34.26%\n",
      "💾 Saved new best model at epoch 6 with accuracy 34.26%\n",
      "Epoch 7/10 | Train Loss: 0.4121, Train Acc: 34.50%\n",
      "💾 Saved new best model at epoch 7 with accuracy 34.50%\n",
      "Epoch 8/10 | Train Loss: 0.3921, Train Acc: 38.35%\n",
      "💾 Saved new best model at epoch 8 with accuracy 38.35%\n",
      "Epoch 9/10 | Train Loss: 0.3655, Train Acc: 38.86%\n",
      "💾 Saved new best model at epoch 9 with accuracy 38.86%\n",
      "Epoch 10/10 | Train Loss: 0.3485, Train Acc: 40.14%\n",
      "💾 Saved new best model at epoch 10 with accuracy 40.14%\n",
      "Best model saved with accuracy: 40.14%\n",
      "✅ Loaded saved model from disk.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁</td></tr><tr><td>Train Accuracy</td><td>▁▃▅▆▆▇▇███</td></tr><tr><td>Train Loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>0.37894</td></tr><tr><td>Train Accuracy</td><td>40.13608</td></tr><tr><td>Train Loss</td><td>0.34854</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">best_model_test_eval</strong> at: <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/transliteration_evaluation/runs/3ae1svie' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/transliteration_evaluation/runs/3ae1svie</a><br> View project at: <a href='https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/transliteration_evaluation' target=\"_blank\">https://wandb.ai/harshtrivs-indian-institute-of-technology-madras/transliteration_evaluation</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250519_070301-3ae1svie\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4502 rows, eval accuracy = 0.38\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "        'embedding_size':256,\n",
    "        'hidden_size': 256,\n",
    "        'enc_layers': 3,\n",
    "        'dec_layers': 3,\n",
    "        'rnn_type': 'GRU',\n",
    "        'dropout_rate': 0.3,\n",
    "        'batch_size': 64,\n",
    "        'epochs':10,\n",
    "        'is_bidirectional':False,\n",
    "        'learning_rate': 0.001,\n",
    "        'optimizer': 'nadam',\n",
    "        'teacher_forcing_prob':0.7,\n",
    "        'beam_size': 5,\n",
    "    }\n",
    "# model_eval(parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
