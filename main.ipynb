{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a2b5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_vocab_size, target_vocab_size,\n",
    "                 embedding_dim=64, hidden_dim=128, rnn_type='LSTM',\n",
    "                 num_layers=1, dropout=0.2):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, embedding_dim)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, embedding_dim)\n",
    "\n",
    "        RNN = {'LSTM': nn.LSTM, 'GRU': nn.GRU, 'RNN': nn.RNN}[rnn_type]\n",
    "\n",
    "        self.encoder_rnn = RNN(embedding_dim, hidden_dim, num_layers,\n",
    "                               batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.decoder_rnn = RNN(embedding_dim, hidden_dim, num_layers,\n",
    "                               batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, target_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        embedded_src = self.dropout(self.encoder_embedding(src))\n",
    "        encoder_outputs, hidden = self.encoder_rnn(embedded_src)\n",
    "\n",
    "        embedded_tgt = self.dropout(self.decoder_embedding(tgt))\n",
    "        decoder_outputs, _ = self.decoder_rnn(embedded_tgt, hidden)\n",
    "\n",
    "        output = self.fc(decoder_outputs)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, file_path, input_char2idx, target_char2idx, max_len=30):\n",
    "        self.pairs = []\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                x, y = parts[1], parts[0]\n",
    "                x_idx = [input_char2idx[c] for c in x if c in input_char2idx]\n",
    "                y_idx = [target_char2idx['<s>']] + [target_char2idx[c] for c in y if c in target_char2idx] + [target_char2idx['</s>']]\n",
    "                self.pairs.append((x_idx[:max_len], y_idx[:max_len]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.pairs[idx]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_padded = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
    "    tgt_padded = nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
    "    return src_padded, tgt_padded\n",
    "\n",
    "\n",
    "def build_vocab(data_path):\n",
    "    input_chars = set()\n",
    "    target_chars = {'<pad>', '<s>', '</s>'}\n",
    "    with open(data_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            x, y = parts[1], parts[0]\n",
    "            input_chars.update(list(x))\n",
    "            target_chars.update(list(y))\n",
    "\n",
    "    input_char2idx = {c: i+1 for i, c in enumerate(sorted(input_chars))}\n",
    "    input_char2idx['<pad>'] = 0\n",
    "\n",
    "    target_char2idx = {c: i for i, c in enumerate(sorted(target_chars))}\n",
    "    target_idx2char = {i: c for c, i in target_char2idx.items()}\n",
    "    return input_char2idx, target_char2idx, target_idx2char\n",
    "\n",
    "\n",
    "def calculate_char_accuracy(preds, targets):\n",
    "    preds = preds.argmax(dim=-1)\n",
    "    match = (preds == targets).float()\n",
    "    mask = (targets != 0).float()\n",
    "    return (match * mask).sum() / mask.sum()\n",
    "\n",
    "def calculate_word_accuracy(preds, targets,target_idx2char, pad_idx=0, eos_idx=None):\n",
    "    \"\"\"\n",
    "    Strict word-level accuracy where entire sequence must match exactly.\n",
    "    Counts as correct ONLY if all non-padding tokens match exactly.\n",
    "    \"\"\"\n",
    "    preds = preds.argmax(dim=-1)\n",
    "    mask = targets != pad_idx\n",
    "    \n",
    "    if eos_idx is not None:\n",
    "        eos_mask = (targets == eos_idx).cumsum(dim=1) <= 1\n",
    "        mask = mask & eos_mask\n",
    "    \n",
    "    correct_words = ((preds == targets) | ~mask).all(dim=1)\n",
    "    # Print pairs of correct words (predicted and target) for strict word-level accuracy\n",
    "    for i, is_correct in enumerate(correct_words):\n",
    "        if is_correct:\n",
    "            pred_word = ''.join([target_idx2char[idx.item()] for idx in preds[i] if idx.item() != pad_idx and (eos_idx is None or idx.item() != eos_idx)])\n",
    "            tgt_word = ''.join([target_idx2char[idx.item()] for idx in targets[i] if idx.item() != pad_idx and (eos_idx is None or idx.item() != eos_idx)])\n",
    "            print(f\"Correct: pred='{pred_word}' tgt='{tgt_word}'\")\n",
    "\n",
    "    return correct_words.float().mean().item()\n",
    "\n",
    "\n",
    "\n",
    "def predict(model, word, input_char2idx, target_idx2char, target_char2idx, device, max_len=30):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Convert input word to indices and pad\n",
    "        src = [input_char2idx.get(c, 0) for c in word]\n",
    "        src = src[:max_len] + [input_char2idx['<pad>']] * (max_len - len(src))\n",
    "        src_tensor = torch.tensor([src], dtype=torch.long).to(device)\n",
    "\n",
    "        # Initial decoder input with <s>\n",
    "        tgt_input = torch.tensor([[target_char2idx['<s>']]], dtype=torch.long).to(device)\n",
    "\n",
    "        output_seq = []\n",
    "\n",
    "        # Encode input\n",
    "        embedded_src = model.dropout(model.encoder_embedding(src_tensor))\n",
    "        encoder_outputs, hidden = model.encoder_rnn(embedded_src)\n",
    "\n",
    "        # Decode step-by-step\n",
    "        for _ in range(max_len):\n",
    "            embedded_tgt = model.dropout(model.decoder_embedding(tgt_input))\n",
    "            decoder_outputs, hidden = model.decoder_rnn(embedded_tgt, hidden)\n",
    "            output_logits = model.fc(decoder_outputs[:, -1, :])\n",
    "            pred_token = output_logits.argmax(dim=-1).item()\n",
    "\n",
    "            if target_idx2char[pred_token] == '</s>':\n",
    "                break\n",
    "            output_seq.append(target_idx2char[pred_token])\n",
    "\n",
    "            # Prepare input for next step\n",
    "            tgt_input = torch.tensor([[pred_token]], dtype=torch.long).to(device)\n",
    "\n",
    "        return ''.join(output_seq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a21c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 2.0101 | Char Acc: 0.4389 | Word Acc: 0.0438\n",
      "Epoch 2/5\n",
      "Train Loss: 0.7791 | Char Acc: 0.7538 | Word Acc: 0.2270\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5643 | Char Acc: 0.8187 | Word Acc: 0.3322\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4618 | Char Acc: 0.8506 | Word Acc: 0.3983\n",
      "Epoch 5/5\n",
      "Train Loss: 0.3975 | Char Acc: 0.8709 | Word Acc: 0.4491\n",
      "\n",
      "Testing the model:\n",
      "ankganit -> अंकनगिताएंगायोंत्रीयामीवार्णीत\n",
      "ankur -> अंकुर्करीनोंडीयरूईएंटीआईवीजीआर\n",
      "angbhang -> अंगभंग्वांतोंगीयारीपोंड़ाईटीआई\n",
      "anguliyon -> अंगुलियोंकोंशीयरोंचीयरुपीयरीवा\n",
      "anguthe -> अंगूठेंटेयाईवीज़ीयोंपीयारीवींत\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Path to your dataset\n",
    "train_path = \"dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
    "\n",
    "# Build vocabulary\n",
    "input_char2idx, target_char2idx, target_idx2char = build_vocab(train_path)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TransliterationDataset(train_path, input_char2idx, target_char2idx)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize model\n",
    "model = Seq2Seq(\n",
    "    input_vocab_size=len(input_char2idx),\n",
    "    target_vocab_size=len(target_char2idx),\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=256,\n",
    "    rnn_type='LSTM',\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore padding\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_char_acc = 0\n",
    "    total_word_acc = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for src, tgt in train_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        # Forward pass with teacher forcing\n",
    "        outputs = model(src, tgt[:, :-1])\n",
    "        loss = criterion(outputs.reshape(-1, outputs.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        char_acc = calculate_char_accuracy(outputs, tgt[:, 1:])\n",
    "        word_acc = calculate_word_accuracy(outputs, tgt[:, 1:], \n",
    "                                          eos_idx=target_char2idx.get('</s>', None),target_idx2char=target_idx2char)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_char_acc += char_acc\n",
    "        total_word_acc += word_acc\n",
    "        batch_count += 1\n",
    "        \n",
    "    # Log metrics\n",
    "    avg_loss = total_loss / batch_count\n",
    "    avg_char_acc = total_char_acc / batch_count\n",
    "    avg_word_acc = total_word_acc / batch_count\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {avg_loss:.4f} | Char Acc: {avg_char_acc:.4f} | Word Acc: {avg_word_acc:.4f}\")\n",
    "\n",
    "# Testing\n",
    "test_words = [\"ankganit\", \"ankur\", \"angbhang\", \"anguliyon\", \"anguthe\"]\n",
    "print(\"\\nTesting the model:\")\n",
    "for word in test_words:\n",
    "    pred = predict(model, word, input_char2idx, target_idx2char, target_char2idx, device)\n",
    "    print(f\"{word} -> {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c40620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        wandb.run.name = (\n",
    "            f\"embedding_dim_{config.embedding_dim}_\"\n",
    "            f\"hidden_dim_{config.hidden_dim}_\"\n",
    "            f\"rnn_type_{config.rnn_type}_\"\n",
    "            f\"num_layers_{config.num_layers}_\"\n",
    "            f\"dropout_{config.dropout}\"\n",
    "\n",
    "        ) # Set the run name based on hyperparameters\n",
    "\n",
    "        train_path = \"dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
    "        dev_path = \"dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\n",
    "\n",
    "        input_char2idx, target_char2idx, target_idx2char = build_vocab(train_path)\n",
    "        train_dataset = TransliterationDataset(train_path, input_char2idx, target_char2idx)\n",
    "        dev_dataset = TransliterationDataset(dev_path, input_char2idx, target_char2idx)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "        dev_loader = DataLoader(dev_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "        model = Seq2Seq(\n",
    "            input_vocab_size=len(input_char2idx),\n",
    "            target_vocab_size=len(target_char2idx),\n",
    "            embedding_dim=config.embedding_dim,\n",
    "            hidden_dim=config.hidden_dim,\n",
    "            rnn_type=config.rnn_type,\n",
    "            num_layers=config.num_layers,\n",
    "            dropout=config.dropout\n",
    "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        device = next(model.parameters()).device\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "        for epoch in range(5):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for src, tgt in train_loader:\n",
    "                src, tgt = src.to(device).long(), tgt.to(device).long()\n",
    "                tgt_input = tgt[:, :-1]\n",
    "                tgt_output = tgt[:, 1:]\n",
    "\n",
    "                logits = model(src, tgt_input)\n",
    "                loss = criterion(logits.reshape(-1, logits.shape[-1]), tgt_output.reshape(-1))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            # Evaluate\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                char_accs = []\n",
    "                word_accs = []\n",
    "                for src, tgt in dev_loader:\n",
    "                    src, tgt = src.to(device).long(), tgt.to(device).long()\n",
    "                    logits = model(src, tgt[:, :-1])\n",
    "                    char_acc = calculate_char_accuracy(logits, tgt[:, 1:])\n",
    "                    word_acc = calculate_word_accuracy(logits, tgt[:, 1:])\n",
    "                    char_accs.append(char_acc.item())\n",
    "                    word_accs.append(word_acc)\n",
    "\n",
    "            val_char_acc = sum(char_accs) / len(char_accs)\n",
    "            val_word_acc = sum(word_accs) / len(word_accs)\n",
    "            wandb.log({\n",
    "                'epoch': epoch+1, \n",
    "                'train_loss': total_loss / len(train_loader), \n",
    "                'val_char_accuracy': val_char_acc, \n",
    "                'val_word_accuracy': val_word_acc\n",
    "            })\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {'name': 'val_word_accuracy', 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'embedding_dim': {'values': [32, 64, 128]},\n",
    "        'hidden_dim': {'values': [64, 128, 256]},\n",
    "        'rnn_type': {'values': ['LSTM', 'GRU','RNN']},\n",
    "        'num_layers': {'values': [1, 2]},\n",
    "        'dropout': {'values': [0.2, 0.3]}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize wandb and run the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"dakshina-transliteration-DA6401\")\n",
    "wandb.agent(sweep_id, function=train, count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4535d812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
